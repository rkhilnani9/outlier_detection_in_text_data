{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Goal is to find outliers in a corpus of addresses.\n",
    "2. This notebook contains unsupervised approaches - Clustering and Rule-Based models.\n",
    "3. DBScan and KModes clustering is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import optimus\n",
    "from optimus.functions import filter_row_by_data_type as fbdt\n",
    "import pyspark.sql.functions as F\n",
    "import dqtool\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Process\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from kmodes.kmodes import KModes\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offices_data_processed = pd.read_csv(\"offices_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_binary_relation(df, col1, col2, thres_1_1=85):\n",
    "\n",
    "    data = df[[col1, col2]].drop_duplicates([col1, col2])\n",
    "    data = data.dropna()\n",
    "    num_cat_col1 = data[col1].nunique()\n",
    "    num_cat_col2 = data[col2].nunique()\n",
    "\n",
    "    # per value of col1, distinct count of col2 values\n",
    "    df_gb_col1 = pd.DataFrame(data.groupby(col1)[col2].nunique()).reset_index()\n",
    "    # per value of col2, distinct count of col1 values\n",
    "    df_gb_col2 = pd.DataFrame(data.groupby(col2)[col1].nunique()).reset_index()\n",
    "\n",
    "    # % of categories having one to one mapping\n",
    "    # 1 -> 2\n",
    "    one_one_col1_perc = len(df_gb_col1[df_gb_col1[col2] == 1]) * 100 / num_cat_col1\n",
    "    # 2 -> 1\n",
    "    one_one_col2_perc = len(df_gb_col2[df_gb_col2[col1] == 1]) * 100 / num_cat_col2\n",
    "\n",
    "    # list of categories having one to many relation\n",
    "    # 1 -> 2\n",
    "    list_flagged_entries_df1 = list(df_gb_col1[df_gb_col1[col2] != 1][col1])\n",
    "    # 2 -> 1\n",
    "    list_flagged_entries_df2 = list(df_gb_col2[df_gb_col2[col1] != 1][col2])\n",
    "\n",
    "    if one_one_col1_perc > thres_1_1 and one_one_col2_perc > thres_1_1:\n",
    "        relation = 'one_to_one'\n",
    "    elif df_gb_col1[col2].mean() > 1 and one_one_col2_perc >= thres_1_1 and num_cat_col1 / num_cat_col2 < 100:\n",
    "        relation = 'parent_col1_child_col2'\n",
    "    elif df_gb_col2[col1].mean() > 1 and one_one_col1_perc >= thres_1_1 and num_cat_col2 / num_cat_col1 < 100:\n",
    "        relation = 'parent_col2_child_col1'\n",
    "    else:\n",
    "        relation = 'none'\n",
    "\n",
    "    if relation == 'one_to_one':\n",
    "        return {'relation': relation, 'flagged': {col1: list_flagged_entries_df1, col2: list_flagged_entries_df2}}\n",
    "    elif relation == 'parent_col1_child_col2':\n",
    "        return {'relation': relation, 'flagged': {col2: list_flagged_entries_df2}}\n",
    "    elif relation == 'parent_col2_child_col1':\n",
    "        return {'relation': relation, 'flagged': {col1: list_flagged_entries_df1}}\n",
    "    else:\n",
    "        return {'relation': relation}\n",
    "\n",
    "def find_all_binary_relations(df, columns):\n",
    "    '''\n",
    "    find relationship between all column pairs in columns\n",
    "    :param df:\n",
    "    :param columns:\n",
    "    :return:\n",
    "    '''\n",
    "    rules = []\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i):\n",
    "            col1, col2 = columns[i], columns[j]\n",
    "            res = find_binary_relation(df, col1, col2)\n",
    "            if res['relation'] != 'none':\n",
    "                rules.append({'col1': col1, 'col2':col2, 'relation': res['relation'], 'flagged': res['flagged']})\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules = find_all_binary_relations(offices_data_processed,['zip_cd', 'van_cty_nm', 'st_cd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rule in rules:\n",
    "    flagged_col = list(rule['flagged'].keys())[0]\n",
    "    col = list(set([rule['col1'], rule['col2']]) - set(flagged_col))[0]\n",
    "    print(flagged_col, col)\n",
    "    rule['pairs'] = dict()\n",
    "    for val in rule['flagged'][flagged_col]:\n",
    "        counts = offices_data_processed[offices_data_processed[flagged_col] == val][col].value_counts()\n",
    "        max_times = counts.idxmax()\n",
    "        if counts.max() == counts.min():\n",
    "            others = list(counts.keys())\n",
    "        else:\n",
    "            others = list(set(list(counts.keys())) - set([max_times]))\n",
    "        rule['pairs'][val] = others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rule_results(zp, cty, st):\n",
    "    \n",
    "    rule_break = \"\"\n",
    "    \n",
    "    for rule in rules:\n",
    "        if 'zip_cd' in rule['flagged'].keys():\n",
    "            if zp in rule['flagged']['zip_cd']:\n",
    "                rule_break += rule['col1'] + \" - \" + rule['col2'] + \"; \"\n",
    "        if 'van_cty_nm' in rule['flagged'].keys():\n",
    "            if cty in rule['flagged']['van_cty_nm']:\n",
    "                rule_break += rule['col1'] + \" - \" + rule['col2'] + \"; \"\n",
    "        if 'st_cd' in rule['flagged'].keys():\n",
    "            if st in rule['flagged']['st_cd']:\n",
    "                rule_break += rule['col1'] + \" - \" + rule['col2'] + \"; \"\n",
    "        \n",
    "    if rule_break == \"\":\n",
    "        return np.nan\n",
    "    return rule_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rule in rules:\n",
    "    flagged_col = list(rule['flagged'].keys())[0]\n",
    "    col = list(set([rule['col1'], rule['col2']]) - set(flagged_col))[0]\n",
    "    offices_data_processed[\"Relation \"+ rule['col1']+\"-\"+rule['col2']+\" Check\"] = \\\n",
    "     offices_data_processed.apply(\n",
    "        lambda x: 1 if x[flagged_col] in rule['pairs'].keys() and x[col] in rule['pairs'][x[flagged_col]] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatype Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = optimus.Optimus(options={'spark.driver.memory' : '7g', 'spark.executer.memory' : '7g'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = dqtool.datasource.DataSourceLocal(\"/Users/avarshn5/Desktop/Projects/DataQ/Adr_out/ml_ai_quest_providers/offices_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dqtool.helpers.get_df(engine, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_type_counts = dqtool.pyspark.infer.count_category_dtype(engine, df, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_data_type = {}\n",
    "\n",
    "for col, v in data_type_counts['columns'].items():\n",
    "    typ = v['dtype']\n",
    "    results_data_type[col] = {'type': typ}\n",
    "    \n",
    "    results_data_type[col]['mismatch'] = {}\n",
    "    for k, v in v['details'].items():\n",
    "        if k!=typ and v>0:\n",
    "            temp = (df.h_repartition(col_name=col)\n",
    "                    .select(col).withColumn(\"check\", fbdt(col, data_type=k))\n",
    "                    .filter(F.col(\"check\"))\n",
    "                    .select(col)\n",
    "                    .distinct().collect())\n",
    "            results_data_type[col]['mismatch'][k] = [row[col] for row in temp]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mismatch_results(x):\n",
    "    \n",
    "    res = \"\"\n",
    "    \n",
    "    for col in results_data_type.keys():\n",
    "        for k in results_data_type[col]['mismatch'].keys():\n",
    "            if x[col] in results_data_type[col]['mismatch'][k]:\n",
    "                res += k + \"; \"\n",
    "                \n",
    "    if res==\"\":\n",
    "        return np.nan\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed[\"Data Type Mismatch Check\"] = \\\n",
    "offices_data_processed.apply(\n",
    "    lambda x: get_mismatch_results(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_addr(addr):\n",
    "\n",
    "    url = 'http://apsrp06825:5008/parser'\n",
    "    myobj = {'query': addr}\n",
    "    x = requests.request(method = 'POST', url=url, json = myobj, timeout=10.0)\n",
    "    resp = {}\n",
    "\n",
    "    for comp in x.json():\n",
    "        k = comp['label']\n",
    "        v = comp['value']\n",
    "        resp[k] = v\n",
    "    return resp\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_resp = []\n",
    "with ThreadPoolExecutor(max_workers=1000) as ex:\n",
    "    for i, resp in enumerate(ex.map(identify_addr, offices_data_processed.adr_ln_1_txt)):\n",
    "        resp['id'] = i\n",
    "        addr_resp.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_resp = []\n",
    "for addr in offices_data_processed.adr_ln_1_txt:\n",
    "    resp = identify_addr(addr)\n",
    "    resp['id'] = i\n",
    "    addr_resp.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = Process(target=identify_addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numerical_count(lt):\n",
    "    nc = 0\n",
    "    for el in lt:\n",
    "        if el.isnumeric():\n",
    "            nc += 1\n",
    "    return nc\n",
    "\n",
    "def get_feature(addr):\n",
    "    addr = addr.strip()\n",
    "    features = []\n",
    "    features.append(len(addr))\n",
    "    features.append(len(addr.split(\" \")))\n",
    "    features.append(get_numerical_count(addr.split(\" \")))\n",
    "    \n",
    "    is_start_with_number_or_po = 0\n",
    "    if addr.split(\" \")[0].isnumeric() \\\n",
    "        or addr.split(\" \")[0][:-1].isnumeric() \\\n",
    "        or addr.split(\" \")[0] in [\"PO\", \"P.O.\"]:\n",
    "        is_start_with_number_or_po = 1\n",
    "    features.append(is_start_with_number_or_po)\n",
    "    \n",
    "    non_obvious_char_count = 0\n",
    "    for c in addr:\n",
    "        if not (c.isalpha() or c.isnumeric() or c in [\" \", \"-\", \"#\", \"&\", \"/\", \".\", \",\"]):\n",
    "            non_obvious_char_count += 1\n",
    "    \n",
    "    features.append(non_obvious_char_count)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adr_ln_1_txt_features = offices_data_processed['adr_ln_1_txt'].apply(lambda x: get_feature(str(x))).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(adr_ln_1_txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = []\n",
    "for i in [2, 4, 6, 8, 10]:\n",
    "    db = DBSCAN(eps=i, min_samples=10).fit(X)\n",
    "    labels = db.labels_\n",
    "\n",
    "    counts.append(pd.Series(labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=10, min_samples=10).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "counts.append(pd.Series(labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_data = offices_data_processed['adr_ln_1_txt'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,l in enumerate(labels):\n",
    "    if l != 0:\n",
    "        print(i,l, out_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_dbscan_outlier(row, i):\n",
    "    if labels[i] == 1:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed['DBScan_Outlier'] = offices_data_processed.apply(lambda x: is_dbscan_outlier(x, x.name), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(adr_ln_1_txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = KModes(n_clusters=15).fit(X)\n",
    "labels_kmode = db.labels_\n",
    "\n",
    "pd.Series(labels_kmode).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,l in enumerate(labels_kmode):\n",
    "    if l == 14:\n",
    "        print(i,l, out_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_kmode_outlier(row, i):\n",
    "    if labels_kmode[i] == 1:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed['KMODE_Outlier'] = offices_data_processed.apply(lambda x: is_kmode_outlier(x, x.name), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip Code Length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_zip_len_5(zp):\n",
    "    if math.isnan(zp):\n",
    "        return np.nan\n",
    "    \n",
    "    if len(str(int(zp))) == 5:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed['zip_cd'] = offices_data_processed['zip_cd'].astype('Int32')\n",
    "offices_data_processed[\"Zip Length 5\"] = offices_data_processed['zip_cd'].apply(lambda x: is_zip_len_5(x))                                                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip Code Length 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_zip_4(zp):\n",
    "    if math.isnan(zp):\n",
    "        return np.nan\n",
    "\n",
    "    if len(str(int(zp))) == 4:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed[\"ZIP + 4 Code is length 4\"] = offices_data_processed['zip_pls_4_cd'].apply(lambda x: is_zip_4(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Address line 1 has PO Box then adr_typ_desc != PLACE OF SERVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def po_adr_type_check(addr, adr_typ):\n",
    "    \n",
    "    if ('PO ' in addr \\\n",
    "        or 'P0 ' in addr \\\n",
    "        or 'P.O. ' in addr \\\n",
    "        or 'P.O ' in addr \\\n",
    "        or 'P O ' in addr ) \\\n",
    "        and 'BOX' in addr.upper():\n",
    "        if adr_typ == 'PLACE OF SERVICE':\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed['PO Box with Place of service check'] = offices_data_processed.apply(lambda x: \n",
    "                                                                                            po_adr_type_check(x['adr_ln_1_txt'], x['adr_typ_desc']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def po_adr(addr):\n",
    "    \n",
    "    if 'PO ' in addr \\\n",
    "        or 'P0 ' in addr \\\n",
    "        or 'P.O. ' in addr \\\n",
    "        or 'P.O ' in addr \\\n",
    "        or 'P O ' in addr:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed['PO Box'] = offices_data_processed['adr_ln_1_txt'].apply(lambda x: \n",
    "                                                                                      po_adr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed = offices_data_processed.drop(columns=['PO Box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offices_data_processed['Number of check failed'] =  offices_data_processed.apply(lambda x: x['Relation van_cty_nm-zip_cd Check']+\n",
    "                                                                                 x['Relation st_cd-zip_cd Check']+\n",
    "                                                                                 x['Relation st_cd-van_cty_nm Check']+\n",
    "                                                                                 x['PO Box with Place of service check']+\n",
    "                                                                                 x['DBScan_Outlier'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "offices_data_processed[offices_data_processed['Number of check failed'] > 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
