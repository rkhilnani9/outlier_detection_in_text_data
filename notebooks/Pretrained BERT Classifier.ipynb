{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Goal is to find outliers in a corpus of addresses.\n",
    "2. This notebook contains supervised approaches - Corrupting data and generating fake data.\n",
    "3. BERT pre-trained model is trained in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T10:11:55.248311Z",
     "start_time": "2020-01-13T10:11:48.010662Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install transformers\n",
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !pip install torch\n",
    "#!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T10:12:11.097106Z",
     "start_time": "2020-01-13T10:12:06.880558Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from scipy import sparse\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from math import log\n",
    "import operator\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import math\n",
    "import spacy\n",
    "torch.manual_seed(123)\n",
    "\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "np.random.seed(123)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import codecs\n",
    "import string\n",
    "from scipy import sparse\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import math\n",
    "import torch.utils.data as data_utils\n",
    "import transformers\n",
    "\n",
    "import faker\n",
    "from faker import Faker\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, config_class, pretrained_weights = DistilBertModel, DistilBertTokenizer, DistilBertConfig, 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "config = config_class.from_pretrained(pretrained_weights,\n",
    "                                      num_labels=2)\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights,\n",
    "                                            do_lower_case=True)\n",
    "bert_model = model_class.from_pretrained(pretrained_weights,\n",
    "                                    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('offices_data.csv')\n",
    "addresses = df_data['adr_ln_1_txt'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corruption Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Corrupting the data by randomly dropping one word from 20 percent of the corpus (also chosen randomly)\n",
    "#The corrupted data points are labelled as 0 and and the normal data is labelled as 1\n",
    "#A neural network using pre-trained BERT Embeddings is trained to predict 0s (the outliers) and 1s\n",
    "#Recall is optimized for outlier class using some rules if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corrupt_data(corpus, corrupt_sample_size):\n",
    "    corrupt_indices = sorted(random.sample(range(len(corpus)), corrupt_sample_size))\n",
    "    corrupt_corpus =[corpus[i] for i in range(len(corpus)) if i in corrupt_indices] \n",
    "    real_corpus = [corpus[i] for i in range(len(corpus)) if i not in corrupt_indices] \n",
    "    corrupt_corpus_tokenized = []\n",
    "    for address in corrupt_corpus:\n",
    "        tokens = address.split(\" \")\n",
    "        num_words = len(address.split(\" \"))\n",
    "        index = np.random.randint(0, num_words, size = 1)\n",
    "        corrupt_corpus_tokenized.append(\" \".join([token for idx, token in enumerate(tokens) if idx not in index]))\n",
    "#       dropped_index_length = int(len(address)/5)\n",
    "#       index = np.random.randint(0, len(address), size = dropped_index_length)\n",
    "#       corrupt_corpus_tokenized.append(\"\".join([char for idx, char in enumerate(address) if idx not in index]))\n",
    "    return real_corpus, corrupt_corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data_for_model(real_corpus, corrupt_corpus, val_sample_size, test_sample_size, corpus):\n",
    "    df_real = pd.DataFrame()\n",
    "    df_corrupt = pd.DataFrame()\n",
    "    df_real['corpus'] = real_corpus\n",
    "    df_real['label'] = 1\n",
    "    #corrupt_corpus_word.extend(corrupt_corpus_char)\n",
    "    df_corrupt['corpus'] = corrupt_corpus\n",
    "    df_corrupt['label'] = 0\n",
    "    df_for_model = pd.concat([df_real, df_corrupt], axis = 0)\n",
    "    df_for_model = df_for_model.sample(frac = 1)\n",
    "    df_for_model['corpus'] = df_for_model['corpus'].str.lower()\n",
    "    x = df_for_model['corpus'].values\n",
    "    y = df_for_model['label'].values\n",
    "    test_indices = sorted(random.sample(range(len(corpus)), test_sample_size))\n",
    "    test_x = [x[i] for i in range(len(corpus)) if i in test_indices]\n",
    "    test_y = [y[i] for i in range(len(corpus)) if i in test_indices]\n",
    "    x = [x[i] for i in range(len(corpus)) if i not in test_indices]\n",
    "    y = [y[i] for i in range(len(corpus)) if i not in test_indices]\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_x, val_x, train_y, val_y = train_test_split(x, y, test_size = val_sample_size)\n",
    "    return train_x, val_x, test_x, train_y, val_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_corpus, corrupt_corpus = corrupt_data(addresses, int(len(addresses)/5))\n",
    "train_x, val_x, test_x, train_y, val_y, test_y = make_data_for_model(real_corpus, corrupt_corpus, int(len(addresses)/5), int(len(addresses)/10), addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus, dev_corpus, test_corpus, y, dev_y, test_y = train_x, val_x, test_x, train_y, val_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tokenized = corpus\n",
    "dev_corpus_tokenized = dev_corpus\n",
    "test_corpus_tokenized = test_corpus\n",
    "for i in range(len(corpus)):\n",
    "    try:\n",
    "        corpus_tokenized[i] = tokenizer.tokenize(corpus[i])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for i in range(len(dev_corpus)):\n",
    "    try:\n",
    "        dev_corpus_tokenized[i] = tokenizer.tokenize(dev_corpus[i])\n",
    "    except:\n",
    "        pass\n",
    "for i in range(len(test_corpus)):\n",
    "    try:\n",
    "        test_corpus_tokenized[i] = tokenizer.tokenize(test_corpus[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(corpus):\n",
    "    input_ids_list = []\n",
    "    segment_ids_list = []\n",
    "    input_mask_list = []\n",
    "    max_seq_length = 64\n",
    "    faulty_index = []\n",
    "    min_idx = 0\n",
    "    for i in range(len(corpus)):\n",
    "        to_append = [\"[CLS]\"] + corpus[i] + [\"[SEP]\"] \n",
    "        segment_ids = [0] * (len(corpus[i]) + 2) \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(to_append)\n",
    "        for idx in input_ids:\n",
    "            if idx > min_idx:\n",
    "                min_idx = idx\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        assert (len(input_ids) == max_seq_length)\n",
    "        assert (len(input_mask) == max_seq_length)\n",
    "        assert (len(segment_ids) == max_seq_length)\n",
    "        input_ids_list.append(input_ids)\n",
    "        segment_ids_list.append(segment_ids)\n",
    "        input_mask_list.append(input_mask)\n",
    "    print (min_idx)\n",
    "\n",
    "    return input_ids_list, segment_ids_list, input_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list, segment_ids_list, input_mask_list = vectorize(corpus_tokenized)\n",
    "input_ids_list2, segment_ids_list2, input_mask_list2 = vectorize(dev_corpus_tokenized)\n",
    "input_ids_list_test, segment_ids_list_test, input_mask_list_test = vectorize(test_corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "dev_y = np.array(dev_y)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "y = y[..., np.newaxis]\n",
    "dev_y = dev_y[..., np.newaxis]\n",
    "test_y = test_y[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ids_list, segment_ids_list, input_mask_list = np.array(input_ids_list), np.array(segment_ids_list), np.array(input_mask_list)\n",
    "input_ids_list2, segment_ids_list2, input_mask_list2 = np.array(input_ids_list2), np.array(segment_ids_list2), np.array(input_mask_list2)\n",
    "input_ids_list_test, segment_ids_list_test, input_mask_list_test = np.array(input_ids_list_test), np.array(segment_ids_list_test), np.array(input_mask_list_test)\n",
    "\n",
    "\n",
    "train_dset = data_utils.TensorDataset(torch.from_numpy(input_ids_list).to(device), torch.from_numpy(segment_ids_list).to(device), torch.from_numpy(input_mask_list).to(device), torch.from_numpy(y).to(device))\n",
    "train_loader = data_utils.DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_dset = data_utils.TensorDataset(torch.from_numpy(input_ids_list2).to(device), torch.from_numpy(segment_ids_list2).to(device), torch.from_numpy(input_mask_list2).to(device))\n",
    "val_loader = data_utils.DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=32\n",
    ")\n",
    "test_dset = data_utils.TensorDataset(torch.from_numpy(input_ids_list_test).to(device), torch.from_numpy(segment_ids_list_test).to(device), torch.from_numpy(input_mask_list_test).to(device))\n",
    "test_loader = data_utils.DataLoader(\n",
    "    test_dset, batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassifierDBert(nn.Module):\n",
    "    def __init__(self, bert_model, dropout_p):\n",
    "        super(ClassifierDBert, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.pre_classifier = nn.Linear(768,768)\n",
    "        self.hidden2label = nn.Linear(768, 2)\n",
    "#         self.hidden2confidence = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, sentence1, segment_ids1, input_mask1):\n",
    "        x1 = self.bert(sentence1, attention_mask=input_mask1)[0]\n",
    "        x1 = x1[:,0]\n",
    "        x1 = self.dropout(nn.ReLU()(self.pre_classifier(x1)))\n",
    "        y  = torch.log_softmax(self.hidden2label(x1), dim = 1)\n",
    "#         score  = torch.log_softmax(self.hidden2confidence(x1), dim = 0)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ClassifierDBert(bert_model, 0.6)\n",
    "\n",
    "# loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-5)\n",
    "no_up = 0\n",
    "EPOCH = 10\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_function, optimizer, epoch_num):\n",
    "    from sklearn.metrics import f1_score\n",
    "    model.train() \n",
    "    avg_loss = 0.0\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    pred_probs_list_train = []\n",
    "    for input_ids, segment_ids, input_mask, label in dataloader:\n",
    "        input_ids, segment_ids, input_mask, label = input_ids.to(\"cuda\"), segment_ids.to(\"cuda\"), input_mask.to(\"cuda\"), label.to(\"cuda\")\n",
    "        model.to(device)\n",
    "        pred = model(input_ids, segment_ids, input_mask)\n",
    "        pred_prob = pred[:, 1].detach().data.cpu().numpy()\n",
    "        pred_probs_list_train.append(np.exp(pred_prob))\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_label = pred.data.max(1)[1].cpu()\n",
    "        pred_res += [pred_label]\n",
    "        truth_res += [label.detach().data.cpu()]\n",
    "        avg_loss += loss.detach().data.item()\n",
    "        count += 1\n",
    "        if count % 5000 == 0:\n",
    "            print('[TRAIN] epoch: %d iterations: %d loss :%g' % (epoch_num, count, loss.detach().data.item()))\n",
    "\n",
    "    avg_loss /= len(input_ids_list)\n",
    "    print('[TRAIN] epoch: %d done! \\n train avg_loss:%g , f1:%g'%(epoch_num, avg_loss, f1_score(torch.cat(truth_res),torch.cat(pred_res), average = 'macro')))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataloader, loss_function, optimizer, epoch_num):\n",
    "    from sklearn.metrics import f1_score\n",
    "    model.eval()\n",
    "    #avg_loss = 0.0\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    pred_probs_list = []\n",
    "    for input_ids, segment_ids, input_mask in dataloader:\n",
    "        input_ids, segment_ids, input_mask = input_ids.to(\"cuda\"), segment_ids.to(\"cuda\"), input_mask.to(\"cuda\")\n",
    "        model.to(device)\n",
    "        pred = model(input_ids, segment_ids, input_mask)\n",
    "        #loss = loss_function(pred, label.view(-1))\n",
    "        pred_prob = pred[:, 1].detach().data.cpu().numpy()\n",
    "        pred_probs_list.append(np.exp(pred_prob))\n",
    "        #pred_probs = np.argmax(pred_probs, axis=1)\n",
    "        #pred_probs_list += [pred_probs]\n",
    "        pred_label = pred.data.max(1)[1].cpu()\n",
    "        pred_res += [pred_label]\n",
    "        #truth_res += [label.detach().data.cpu()]\n",
    "        #avg_loss += loss.detach().data.item()\n",
    "        #count += 1\n",
    "    #avg_loss /= len(input_ids_list)\n",
    "    print('[EVAL] epoch: %d done!'%(epoch_num))\n",
    "    return pred_probs_list, pred_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "epoch = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_epoch(model, train_loader, loss_function, optimizer, epoch)\n",
    "    pred_probs_list, pred_res = eval_epoch(model, val_loader, loss_function, optimizer, epoch)\n",
    "    pred_probs_list = np.concatenate(pred_probs_list).ravel()\n",
    "    pred_res = np.concatenate(pred_res).ravel()\n",
    "    print(roc_auc_score(dev_y, pred_probs_list), f1_score(dev_y, pred_res))\n",
    "    pred_probs_list_test ,pred_res_test  = eval_epoch(model, test_loader, loss_function, optimizer, epoch)\n",
    "    pred_probs_list_test = np.concatenate(pred_probs_list_test).ravel()\n",
    "    pred_res_test = np.concatenate(pred_res_test).ravel()\n",
    "    print(roc_auc_score(test_y, pred_probs_list_test), f1_score(test_y, pred_res_test))\n",
    "    print(classification_report(test_y, pred_res_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Data Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating fake data and corrupting them by dropping some words and characters at random\n",
    "#Followed the same approach as data corruption after the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_addresses = []\n",
    "test_addresses = []\n",
    "for _ in range(len(train_addresses)):\n",
    "    val_addresses.append(fake.address())\n",
    "    test_addresses.append(fake.address())\n",
    "    val_addresses[idx] = val_addresses[idx].replace('\\n', \" \")\n",
    "    test_addresses[idx] = test_addresses[idx].replace('\\n', \" \")\n",
    "    val_addresses[idx] = val_addresses[idx].replace(\",\", \"\")\n",
    "    test_addresses[idx] = test_addresses[idx].replace(\",\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_addresses = train_addresses.tolist()\n",
    "train_addresses.extend(val_addresses)\n",
    "train_addresses.extend(test_addresses)\n",
    "addresses = train_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle(addresses)\n",
    "\n",
    "real_corpus = addresses[:65757]\n",
    "corrupt_corpus_word = addresses[65757:131514]\n",
    "corrupt_corpus_char = addresses[131514:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrupt_corpus_word_level = []\n",
    "for address in corrupt_corpus_word:\n",
    "    tokens = address.split(\" \")\n",
    "    num_words = len(address.split(\" \"))\n",
    "    index = np.random.randint(0, num_words, size = 1)\n",
    "    corrupt_corpus_word_level.append(\" \".join([token for idx, token in enumerate(tokens) if idx not in index]))\n",
    "\n",
    "corrupt_corpus_char_level = []\n",
    "for address in corrupt_corpus_char:\n",
    "    dropped_index_length = np.random.randint(1, 5, size = 1)\n",
    "    index = np.random.randint(0, len(address), size = dropped_index_length)\n",
    "    corrupt_corpus_char_level.append(\"\".join([char for idx, char in enumerate(address) if idx not in index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Follow the same vectorizing process followed by running the model to complete this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
